{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EVAAssignment2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudhakarmlal/Project1/blob/master/EVAAssignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nJ7YEw_vyjG",
        "colab_type": "text"
      },
      "source": [
        "# ***Assignement2 -EVA***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Urpq_I5ESGMR",
        "colab_type": "text"
      },
      "source": [
        "## **Not an ideal network**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNCkFm5SSowX",
        "colab_type": "text"
      },
      "source": [
        "####  Install Keras using pip install and import keras library required for Convolution and building Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGWv5hBhv2jf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpzoKupSTXZP",
        "colab_type": "text"
      },
      "source": [
        "#### Import other packages e.g 'numpy' and 'np_utils' for numeric operations,'Sequential' to build a sequential network,'Convolution2D' for  2D convolutions,'mnist'  for downloading mnist dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnMlDJQKv4VG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Convolution2D\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGDbIBioURKz",
        "colab_type": "text"
      },
      "source": [
        "#### Split Mnist dataset into  test and train using mnist.load_data() function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CdSu2lMwB9s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJ9pfqhEUdlF",
        "colab_type": "text"
      },
      "source": [
        "#### Visual first element of the train dataset using matplotlib library used for visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLaDf0-rwCmj",
        "colab_type": "code",
        "outputId": "e7335216-e740-4ed7-de73-5c4640f0c2f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "source": [
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[0])"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcfcaa30e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARNkN_DeVInm",
        "colab_type": "text"
      },
      "source": [
        "####  We need to unroll  the width and hieght of the  images to  28 * 28 frame  for both test and train dataset using reshape() function.This would be required since we would like to work on a  input vector 784(28*28) for our Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erb11jNwwFwl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkHKJaZYb_CL",
        "colab_type": "text"
      },
      "source": [
        "#### Plot the pixel value distribution for 1st Element of the train dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3hHKvCxaMnF",
        "colab_type": "code",
        "outputId": "df23516a-fe7e-448d-8c85-0936d236a2fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "fig = plt.figure()\n",
        "plt.subplot(2,1,1)\n",
        "plt.hist(X_train[0].reshape(784))\n",
        "plt.title(\"Pixel Value Distribution\")"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Pixel Value Distribution')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAACSCAYAAABLwAHLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2r2Ri3ascKnB",
        "colab_type": "text"
      },
      "source": [
        "#### Looking at the graph above the pixel value distribution values ranges from 0 to 255 .The background majority close to 0(hence around 600 0's in the above plot) and the values which are close to  255 represents the actual digit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfjHf_h-dWZ2",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4lK5KbwdYjV",
        "colab_type": "text"
      },
      "source": [
        " ####  Reshape our inputs to a single vector vector and normalize the pixel values to lie between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLK4YDoRwHet",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr9RxsT2ecas",
        "colab_type": "text"
      },
      "source": [
        "#### Normalizing the input data helps to speed up the training. Also, it reduces the chance of getting stuck in local optima, since we're using stochastic gradient descent to find the optimal weights for the network.As explained earlier the X_train value after normalizing would  be between  0 and 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFsQpjzUgo9S",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ty0fRqiIgpGq",
        "colab_type": "text"
      },
      "source": [
        "####  Check the values of y_train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNKLOmhlwJQl",
        "colab_type": "code",
        "outputId": "fd8d21e1-06e8-40d0-dcd4-2a73cd9f53f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y_train[:10]"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RG3_cwutg5zI",
        "colab_type": "text"
      },
      "source": [
        "###   One Hot Encoding\n",
        "#### Convert both training and testing labels into one-hot vectors.Use np_utils.to_categorical to convert 1-dimensional class to 10-dimensional classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YusMJguiwKsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KScudZ8vhcqc",
        "colab_type": "text"
      },
      "source": [
        "#### Check the values of Y_train values after the One-Hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upxc99AswMW0",
        "colab_type": "code",
        "outputId": "d031933b-233b-4ef2-f8dd-2a424d44ddea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "Y_train[:10]\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ocLM32ch98D",
        "colab_type": "text"
      },
      "source": [
        "## Build the Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTVUeBtpiGFH",
        "colab_type": "text"
      },
      "source": [
        "#### From keras.layers import Activation and MaxPooling2D for building the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYcH25tOiEys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Activation, MaxPooling2D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7UlCHkciofy",
        "colab_type": "text"
      },
      "source": [
        "#### Define sequential Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqbELLCdiwQU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WvJ5q6oi3x6",
        "colab_type": "text"
      },
      "source": [
        "### First Layer\n",
        "\n",
        "#### Perform 3 * 3 convolution  on the input image of size 28 * 28 i.e perform convolution using  3*3 filter on 28*28 image\n",
        "\n",
        "#### Take 32 3*3  filters  in the 1st layer \n",
        "\n",
        "\n",
        "#### The input channel dimension is: 28 *  28\n",
        "#### The receptive field for the 1st Layer is :3\n",
        "\n",
        "\n",
        "#### The no of input channels for the 1st Layer is :1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IN2rpIDLkvZW",
        "colab_type": "code",
        "outputId": "6d2a2335-2e77-4c2d-b408-d858acda2659",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "model.add(Convolution2D(32, 3, 3, activation='relu', input_shape=(28,28,1)))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyJ8Zvptk8xA",
        "colab_type": "text"
      },
      "source": [
        "###Second Layer\n",
        "\n",
        "####Perform 3 * 3 convolution on the  32 channels  using 3 * 3 filter.\n",
        "####Take 64 3*3 filters\n",
        "\n",
        "\n",
        "#### The input channel dimension is      : 26 *  26\n",
        "####The receptive field for the 2nd Layer is     :5\n",
        "\n",
        "####The  no of input channels for the 2nd Layer is :32"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ihyfKmSm9rz",
        "colab_type": "code",
        "outputId": "69447c35-3f9f-4953-ef92-4f24016f4a6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "model.add(Convolution2D(64, 3, 3, activation='relu'))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1Iz2Zb2nbTV",
        "colab_type": "text"
      },
      "source": [
        "###3rd  Layer\n",
        "\n",
        "####Perform 3 * 3 convolution on the  64 channels  using 3 * 3 filter.\n",
        "####Take 128 3*3 filters\n",
        "\n",
        "\n",
        "#### The input channel dimension  for 3rd layer is : 24*24\n",
        "\n",
        "####The receptive field for the 3rd  Layer is :7\n",
        "\n",
        "####The no of input channels for the 3rd Layer is :64"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9BgHNhyndnq",
        "colab_type": "code",
        "outputId": "d12fa09e-a8ad-467d-9307-bba63ac63117",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "model.add(Convolution2D(128, 3, 3, activation='relu'))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\")`\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYIA97LqnfnK",
        "colab_type": "text"
      },
      "source": [
        "#### Perform max pooling which would reduce the size of the image from  22 * 22  to  11 * 11 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A57nWLl5noO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(MaxPooling2D(pool_size=(2, 2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDy7nBDgnpPN",
        "colab_type": "text"
      },
      "source": [
        "###4th  Layer\n",
        "\n",
        "#### Perform 3 * 3 convolution on the 128 channels using 3 * 3 filter.\n",
        "\n",
        "#### Take 256 3*3 filters\n",
        "\n",
        "#### The input channel dimension for 4th layer is : 11*11\n",
        "\n",
        "#### The receptive field for the 4th Layer is :9\n",
        "\n",
        "#### The no of  input channels for the 4th  Layer is :128"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FORZvgR9nqlI",
        "colab_type": "code",
        "outputId": "52f707c7-8853-4b73-80f5-59a66144fd7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "model.add(Convolution2D(256, 3, 3, activation='relu'))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\")`\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4i7rKLinrHi",
        "colab_type": "text"
      },
      "source": [
        "###5th  Layer\n",
        "\n",
        "#### Perform 3 * 3 convolution on the 256 channels using 3 * 3 filter.\n",
        "\n",
        "#### Take 512 3*3 filters\n",
        "\n",
        "#### The input channel dimension for 5th layer is : 9*9\n",
        "\n",
        "#### The receptive field for the 5th Layer is :11\n",
        "\n",
        "#### The no of input channels for the 5th  Layer is :128"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCJ4pwx7nsyy",
        "colab_type": "code",
        "outputId": "e0c3ac2c-9a49-41d7-8f9e-cc20b063600c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "model.add(Convolution2D(512, 3, 3, activation='relu'))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\")`\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LxLvfwxntov",
        "colab_type": "text"
      },
      "source": [
        "###6th  Layer\n",
        "\n",
        "#### Perform 3 * 3 convolution on the 512 channels using 3 * 3 filter.\n",
        "\n",
        "#### Take 1024 3*3 filters\n",
        "\n",
        "#### The input channel dimension for 6th layer is : 7 * 7\n",
        "\n",
        "#### The receptive field for the 6th Layer is :13\n",
        "\n",
        "#### The no of input channels for the 6th  Layer is :512"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVwnclCBnvLO",
        "colab_type": "code",
        "outputId": "635a7ce4-80ec-4564-f7f5-810319fdf3fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "model.add(Convolution2D(1024, 3, 3, activation='relu'))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1024, (3, 3), activation=\"relu\")`\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDeQu4wdnwQg",
        "colab_type": "text"
      },
      "source": [
        "###7th  Layer\n",
        "\n",
        "#### Perform 3 * 3 convolution on the 1024 channels using 3 * 3 filter.\n",
        "\n",
        "#### Take 2048 3*3 filters\n",
        "\n",
        "#### The input channel dimension for 7th layer is : 5 * 5\n",
        "\n",
        "#### The receptive field for the 7th Layer is :15\n",
        "\n",
        "#### The no of input channels for the 7th  Layer is :1024"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_XukhdJn0b7",
        "colab_type": "code",
        "outputId": "5042da28-dc54-494e-dad3-91b2b506dc13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "model.add(Convolution2D(2048, 3, 3, activation='relu'))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(2048, (3, 3), activation=\"relu\")`\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IizZGJ3iormX",
        "colab_type": "text"
      },
      "source": [
        "###8th  Layer\n",
        "\n",
        "#### Perform 3 * 3 convolution on the 2048 channels using 3 * 3 filter.\n",
        "\n",
        "#### Take 10 3*3 filters\n",
        "\n",
        "#### The input channel dimension for 8th layer is : 3 * 3\n",
        "\n",
        "#### The receptive field for the 8th Layer is :17\n",
        "\n",
        "#### The no of input channels for the 8th  Layer is :2048"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tl2j5E8Vn1nL",
        "colab_type": "code",
        "outputId": "1c54e3d7-463e-44d7-8ed4-bd80ad73f2db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "model.add(Convolution2D(10, 3, 3, activation='relu'))"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\")`\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ4DZuuTo6u2",
        "colab_type": "text"
      },
      "source": [
        "#### Flatten the 10 channels  which is the output of  8th Layer  for the actual output\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MW9OxFdPo97x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGdPNRJwpDSV",
        "colab_type": "text"
      },
      "source": [
        "####  Summarize  the model  using model.summary().\n",
        "#### It shows each layer's details along with the number of parameters used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1IiQ8JHpENs",
        "colab_type": "code",
        "outputId": "57899fcb-8ece-470a-e6a0-8af8382716b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_10 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 24, 24, 64)        18496     \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 22, 22, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 11, 11, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 9, 9, 256)         295168    \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 7, 7, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 5, 5, 1024)        4719616   \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 3, 3, 2048)        18876416  \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 1, 1, 10)          184330    \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 25,348,362\n",
            "Trainable params: 25,348,362\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irTVUE47wNwr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from keras.layers import Activation, MaxPooling2D\n",
        "\n",
        "#model = Sequential() \n",
        "#model.add(Convolution2D(32, 3, 3, activation='relu', input_shape=(28,28,1)))\n",
        "#model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
        "#model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
        "\n",
        "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "#model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
        "#model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
        "#model.add(Convolution2D(1024, 3, 3, activation='relu'))\n",
        "#model.add(Convolution2D(2048, 3, 3, activation='relu'))\n",
        "#model.add(Convolution2D(10, 3, 3, activation='relu'))\n",
        "\n",
        "#model.add(Flatten())\n",
        "#model.add(Activation('softmax'))\n",
        "\n",
        "#model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILIp2zlZzJ5q",
        "colab_type": "text"
      },
      "source": [
        "#### Compile the model.Use categorical_crossentropy as the loss function and adam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYZOpRb6yG7_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzf9Ey7lzW4j",
        "colab_type": "text"
      },
      "source": [
        "#### Train the model for 10 epochs with a batch size of 32"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5O248wVQyMft",
        "colab_type": "code",
        "outputId": "1f2367e7-a879-43fb-ab91-145968b1cb43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        }
      },
      "source": [
        "model.fit(X_train, Y_train, batch_size=32, nb_epoch=10, verbose=1)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 118s 2ms/step - loss: 2.3027 - acc: 0.0989\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 112s 2ms/step - loss: 2.3026 - acc: 0.0987\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 112s 2ms/step - loss: 2.3026 - acc: 0.0987\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 112s 2ms/step - loss: 2.3026 - acc: 0.0987\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 112s 2ms/step - loss: 2.3026 - acc: 0.0987\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 112s 2ms/step - loss: 2.3026 - acc: 0.0987\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 112s 2ms/step - loss: 2.3026 - acc: 0.0987\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 112s 2ms/step - loss: 2.3026 - acc: 0.0987\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 112s 2ms/step - loss: 2.3026 - acc: 0.0987\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 112s 2ms/step - loss: 2.3026 - acc: 0.0987\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcfcb5c75f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fp8YlkPCz9UP",
        "colab_type": "text"
      },
      "source": [
        "#### Calculate the score of the *model*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sst4KneiyOL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMqOqHUxzhbg",
        "colab_type": "text"
      },
      "source": [
        "#### Print the score of the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfJiXOKsyj4y",
        "colab_type": "code",
        "outputId": "60bb24ff-7201-4cc6-c6ba-b4c019a71dd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(score)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.3025851249694824, 0.098]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9M__S65L3By7",
        "colab_type": "text"
      },
      "source": [
        "#### Find the predicted values using model.predict() on the test dataset(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwLSXt7nyn_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0-CHlI2zyQ0",
        "colab_type": "text"
      },
      "source": [
        "#### Print  y_pred(One hot encoded) values and the actual y value for the 1st 10 records in the data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWKKoOKwyppN",
        "colab_type": "code",
        "outputId": "5278b058-4908-467d-9680-e9a67f445fd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "print(y_pred[:9])\n",
        "print(y_test[:9])"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
            " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
            " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
            " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
            " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
            " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
            " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
            " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
            " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]]\n",
            "[7 2 1 0 4 1 4 9 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vLmAP190B_8",
        "colab_type": "text"
      },
      "source": [
        "## Whats Wrong with the Model?\n",
        "\n",
        "#### 1.The Model has unnecessarily used more number of layers where the receptive field is beyond the size of the image.\n",
        "\n",
        "#### 2.As explained above, it would have been good if the model would have four layers where the receptive field is 9 and input dimension of the image is 11.\n",
        "\n",
        "#### 3.There is no point going for more number of layers where receptive field is beyond the size of the image.This would lead the network to look beyond the size of the image and hence we can never achieve optimal results.\n",
        "\n",
        "#### 4.Also its a wastage of processing(cpu/gpu) having redundant layers e.g 5th,6th,7th layers for the case above.\n",
        "\n",
        "#### 5. As the model has read the features, we can think of reducing the number of number of filters for the later layers e.g  4th,5th,6th......That didn't happen in the case above where the no of filters kept increasing from 32 to 64 to 128 to 256 to 512 .... upto  2048.\n",
        "\n",
        "#### 6.More number of filters would mean more processing which may not be feasible with commodity hardware.In case we want more number of filters in the later layers we can run that only on high end configuration machine which would be expensive\n",
        "\n"
      ]
    }
  ]
}